

---
slug: function-signature-as-contract
difficulty: medium
topic: Understanding that a function signature and docstring define a contract between the implementer and every caller
question: |
  A codon optimization function is documented as:

  <python>
  def optimize_codons(cds: str, organism: str) -> str:
      '''Accepts a DNA coding sequence and a host organism name.
      Returns a codon-optimized DNA sequence of the same length.
      Raises ValueError if cds length is not a multiple of 3.'''
      ...
  </python>

  A colleague calls the function and writes these checks:

  <python>
  result = optimize_codons('ATGAAAGGG', 'ecoli')
  assert len(result) == len('ATGAAAGGG')
  assert result.startswith('ATG')
  </python>

  Why is the length assertion justified but the startswith assertion risky?
choices:
  A: The contract guarantees same-length output but makes no promise about preserving the start codon
  B: Both assertions are guaranteed by the contract because ATG always maps to ATG
  C: Neither assertion is safe because a return type of str allows any string
  D: The startswith check is safe because biological convention requires a start codon
answer: A
explanation: |
  The docstring explicitly guarantees that the returned sequence has the same
  length as the input, so the length assertion is contract-safe. The contract
  says nothing about preserving any specific codon, including the start codon.
  A real implementation would usually preserve <pre>ATG</pre>, but callers should not
  rely on behavior the API does not explicitly promise. Option B assumes a
  biological fact that is outside the stated contract. Option C ignores the
  explicit same-length guarantee. Option D confuses biological convention with
  an API guarantee.

---
slug: separate-computation-from-io
difficulty: medium
topic: Recognizing why separating pure computation from I/O makes functions reusable and testable
question: |
  Two approaches to computing the melting temperature of a primer:

  <python>
  # Version A
  def get_tm_from_user():
      seq = input('Enter primer sequence: ')
      tm = 2 * sum(1 for b in seq if b in 'AT') + 4 * sum(1 for b in seq if b in 'GC')
      print(f'Tm = {tm}')

  # Version B
  def compute_tm(seq: str) -> float:
      return 2 * sum(1 for b in seq if b in 'AT') + 4 * sum(1 for b in seq if b in 'GC')
  </python>

  A design pipeline needs to compute Tm for 500 primers stored in a list.
  Which version can it use directly?
choices:
  A: Version A — it can be called in a loop with each primer
  B: Version B — it accepts a sequence argument and returns a value, so it works with any data source
  C: Both work equally well — print and return are interchangeable
  D: Neither — both are unusable because the formula is a simplified approximation
answer: B
explanation: |
  Version A mixes computation with I/O: it reads from the keyboard and prints
  to the console. That makes it hard to use inside a pipeline that already has
  the sequences in memory and needs the numeric result back. Version B cleanly
  separates computation from I/O by taking an input value and returning an
  output value. This makes it reusable in loops, tests, web apps, notebooks,
  and other functions. Option A fails because <python>input()</python> blocks for
  interactive entry. Option C confuses a side effect (<python>print</python>) with
  a returned value. Option D critiques the model, but the question is about API
  design and reusability.

---
slug: pure-function-guarantees
difficulty: medium
topic: Identifying the guarantees a pure function provides and why they matter for reproducible pipelines
question: |
  A gene design pipeline calls three functions in sequence:

  <python>
  def codon_optimize(cds, codon_table):
      '''Pure function: returns optimized CDS.'''
      ...

  def log_to_database(cds, timestamp):
      '''Writes the CDS and timestamp to a remote database.'''
      ...

  def add_flanking_sequences(cds, vector):
      '''Pure function: returns CDS with appropriate flanks.'''
      ...
  </python>

  The pipeline runs once and produces a correct result. The server hosting
  the database later goes offline. What happens when the pipeline is rerun
  with identical inputs?
choices:
  A: All three functions fail — the database outage propagates to the pure functions
  B: codon_optimize and add_flanking_sequences produce the same results; log_to_database fails
  C: The pipeline produces a different result because the database cached state from the first run
  D: All three succeed — Python automatically retries failed network calls
answer: B
explanation: |
  Pure functions depend only on their inputs and have no side effects, so they
  are unaffected by external failures like a database outage. With identical
  inputs, <python>codon_optimize</python> and <python>add_flanking_sequences</python>
  should return the same outputs as before. <python>log_to_database</python>
  performs network I/O and fails when the database is unavailable. This is why
  pipelines are typically designed with a pure computational core and I/O at the
  edges. Option A incorrectly treats pure functions as externally dependent.
  Option C invents a hidden dependency not stated in the code. Option D invents
  automatic retry behavior.

---
slug: mcp-host-client-server-roles
difficulty: medium
topic: Distinguishing the roles of host, client, and server in a layered protocol architecture
question: |
  In the Model Context Protocol (MCP), three roles cooperate:

  - Server: exposes capabilities (Resources, Tools, Prompts)
  - Client: maintains a 1:1 connection with a server
  - Host: the user-facing application that coordinates clients

  A bioinformatics lab runs an MCP server that exposes a codon optimization
  Tool. A researcher uses an LLM-powered assistant (the host) connected to
  that server. The LLM proposes calling the codon optimization tool for the
  sequence <pre>ATGAAACCC</pre>.

  Who decides whether the tool is actually invoked?
choices:
  A: The server — it controls access to all tools it exposes
  B: The LLM — including the tool call in its response already executes it
  C: The host — it mediates between the LLM and the client, applying approval policies
  D: The client — it autonomously executes any request the LLM generates
answer: C
explanation: |
  In MCP, the LLM proposes actions, but the host is responsible for applying
  approval policies before any tool call is executed. The host mediates between
  the user, the model, and the client connection. Option A confuses exposing a
  tool with deciding whether to execute a particular invocation. Option B treats
  a proposal as execution. Option D skips the host approval layer that is
  central to controlled tool use.

---
slug: mcp-resources-vs-tools
difficulty: medium
topic: Distinguishing read-only data access from actions that produce side effects
question: |
  An MCP server for a genetic design platform exposes the following:

  Resource: "parts-catalog" — returns the current list of BioBrick parts
  Resource: "codon-tables" — returns codon usage tables for supported organisms
  Tool: "run-optimization" — accepts a CDS and returns an optimized sequence
  Tool: "order-dna" — submits a DNA sequence to a synthesis vendor

  A student claims that Resources and Tools are interchangeable and that
  "parts-catalog" could just as easily be a Tool. What is wrong with this claim?
choices:
  A: Resources are for read-only data retrieval; Tools are for actions that may have side effects — conflating them weakens the safety model
  B: Resources return JSON and Tools return plain text, so the formats are incompatible
  C: Tools are faster than Resources, which is why computation should be a Tool
  D: Resources can only be accessed by the server itself, not by external clients
answer: A
explanation: |
  MCP separates Resources (read-only retrieval) from Tools (invoked actions that
  may have side effects). Read access to a parts catalog should not require the
  same approval flow as actions like placing an order. Treating read-only data
  as a Tool creates unnecessary approval friction and blurs an important safety
  boundary. Option B invents a format rule. Option C invents a performance rule.
  Option D is false because Resources are intended for client access.

---
slug: tool-invocation-approval
difficulty: medium
topic: Understanding why proposed actions require explicit approval before execution
question: |
  An MCP-connected assistant is helping a researcher design a plasmid.
  The assistant has access to these tools:

  Tool: "blast-sequence" — runs BLAST against NCBI (network call)
  Tool: "order-dna" — submits a sequence to IDT for synthesis ($$$)
  Tool: "write-genbank" — writes a GenBank file to disk

  The LLM generates a plan: "I'll BLAST your insert, write the GenBank file,
  and order the DNA." Under MCP's controlled invocation model, what should
  happen next?
choices:
  A: All three tools execute immediately — the user already asked for help
  B: The host presents the proposed tool calls for approval before any execute
  C: Only order-dna requires approval; blast-sequence and write-genbank run automatically
  D: The server decides which tools are safe and executes those without asking
answer: B
explanation: |
  In MCP, tool calls are proposed first and executed only after host approval
  (by the user or by an explicit policy). That approval step matters because
  the tools here have real side effects: network access, filesystem writes, and
  a purchase/order action. Option A skips the control boundary. Option C may be
  possible under a custom policy, but it is not the default controlled-invocation
  model. Option D incorrectly places the approval decision at the server rather
  than the host.

---
slug: error-modes-raise-vs-return-none
difficulty: medium
topic: Choosing between raising an exception and returning a sentinel value to communicate failure
question: |
  A function validates a DNA sequence before processing:

  <python>
  # Approach 1: raise on error
  def validate_cds(seq):
      if len(seq) % 3 != 0:
          raise ValueError(f'CDS length {len(seq)} is not a multiple of 3')
      return seq

  # Approach 2: return None on error
  def validate_cds(seq):
      if len(seq) % 3 != 0:
          return None
      return seq
  </python>

  A caller passes an invalid sequence and then immediately calls
  <python>translate()</python> on the result without checking:

  <python>
  cds = validate_cds('ATGAA')
  protein = translate(cds)
  </python>

  Which approach leads to a more diagnosable failure?
choices:
  A: Approach 2 — returning None lets the caller decide how to handle the error
  B: Approach 1 — the ValueError halts immediately with a message explaining the problem
  C: Both are equivalent — translate will fail either way
  D: Approach 2 — None is automatically skipped by translate
answer: B
explanation: |
  Approach 1 fails immediately at the point where the invalid input is detected
  and raises a ValueError that explains the actual problem. Approach 2 silently
  returns <python>None</python>; if the caller forgets to check, the eventual
  error appears later inside <python>translate()</python>, often as a less useful
  TypeError or AttributeError. Option A is about flexibility for careful callers,
  but the question asks about diagnosability when the caller does not check.
  Option C ignores the difference in error quality. Option D invents behavior.

---
slug: function-scope-documentation
difficulty: medium
topic: Documenting what a function does and does not handle to set correct expectations
question: |
  A function's docstring states:

  <python>
  def remove_restriction_sites(cds: str, sites: list[str]) -> str:
      '''Silently replaces codons to eliminate the specified restriction
      sites from cds while preserving the encoded protein.

      Scope: operates only on the coding region. Does not modify UTRs,
      promoters, or flanking sequences. Does not check whether the
      replacement introduces new forbidden sites not in the input list.

      Raises ValueError if cds is not a valid reading frame.
      Returns the modified CDS as a string.'''
      ...
  </python>

  A researcher passes a full plasmid sequence (promoter + CDS + terminator)
  and finds that a BsaI site in the promoter is still present after the call.
  What went wrong?
choices:
  A: The function has a bug — it should scan the entire input for restriction sites
  B: The researcher used the function outside its documented scope — it only operates on a coding region
  C: BsaI sites cannot be removed computationally, only by manual design
  D: The function silently failed because the promoter confused its reading-frame parser
answer: B
explanation: |
  The docstring explicitly limits the function's scope to the coding region and
  states that promoters and flanking sequences are not modified. The researcher
  passed a full plasmid, so the function was used outside its documented
  contract. This is a usage error, not evidence of a bug in the stated behavior.
  Option A ignores the documented scope. Option C is factually wrong. Option D
  invents a failure mode not supported by the contract.

---
slug: reference-by-identifier-serialization
difficulty: medium
topic: Understanding why storing identifiers rather than full objects enables reliable serialization
question: |
  A cloning plan is stored as a Python dict and serialized to JSON:

  <python>
  # Approach 1: embed full objects
  plan = {
      'step': 'Gibson',
      'fragments': [polynucleotide_obj_1, polynucleotide_obj_2]
  }

  # Approach 2: reference by identifier
  plan = {
      'step': 'Gibson',
      'fragment_ids': ['frag_001', 'frag_002']
  }
  </python>

  When <python>json.dumps(plan)</python> is called on each approach, what happens?
choices:
  A: Both serialize successfully — json.dumps handles any Python object
  B: Approach 1 raises TypeError because custom objects are not JSON-serializable; approach 2 succeeds because it stores only strings
  C: Approach 2 fails because identifiers cannot be resolved during serialization
  D: Both fail — dicts cannot be serialized to JSON
answer: B
explanation: |
  <python>json.dumps()</python> can serialize standard JSON-compatible Python
  types such as dicts, lists, strings, numbers, booleans, and None. Custom
  objects (like a Polynucleotide instance) are not JSON-serializable by default
  and raise TypeError unless a custom encoder is provided. Storing identifiers
  as strings avoids that problem and cleanly separates serialization from later
  object resolution. Option C confuses serialization with lookup. Option D is
  false because dicts of JSON-compatible values serialize normally.

---
slug: json-schema-contract-enforcement
difficulty: medium
topic: Using a schema as a machine-readable contract that enforces structure before data enters a system
question: |
  A design tool accepts JSON input validated against this schema:

  <pre>
  {
    "type": "object",
    "required": ["sequence", "organism"],
    "properties": {
      "sequence": {"type": "string", "pattern": "^[ATCG]+$"},
      "organism": {"type": "string", "enum": ["ecoli", "yeast", "human"]},
      "circular": {"type": "boolean"}
    },
    "additionalProperties": false
  }
  </pre>

  Which JSON input passes validation?
choices:
  A: <pre>{"sequence": "ATGCCC", "organism": "ecoli"}</pre>
  B: <pre>{"sequence": "ATGCCC", "organism": "zebrafish"}</pre>
  C: <pre>{"sequence": "ATGCCC"}</pre>
  D: <pre>{"sequence": "ATGCCC", "organism": "ecoli", "temperature": 37}</pre>
answer: A
explanation: |
  Option A includes both required fields, uses a sequence that matches the DNA
  pattern, and provides an allowed organism value. <python>circular</python> is
  optional, so omitting it is valid. Option B fails the enum constraint.
  Option C fails the required-field constraint. Option D fails because
  <python>additionalProperties: false</python> disallows undeclared keys such as
  <python>temperature</python>. The schema acts as a machine-readable boundary
  contract by enforcing structure and allowed values before computation begins.