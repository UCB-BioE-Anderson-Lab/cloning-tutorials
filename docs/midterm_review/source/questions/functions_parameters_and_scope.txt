---
slug: return-vs-print-in-design
difficulty: medium
topic: Understanding that functions communicate results through return values, not side effects
question: |
  A student writes a function to translate a CDS into a protein:

  <python>
  def translate(cds, genetic_code):
      protein = ''
      for i in range(0, len(cds), 3):
          codon = cds[i:i+3]
          protein += genetic_code[codon]
      print(protein)

  result = translate('ATGAAATAA', genetic_code)
  </python>

  They then try to check if the protein starts with methionine:

  <python>
  result.startswith('M')
  </python>

  What happens?
choices:
  A: Returns True — the protein starts with 'M'
  B: AttributeError — result is None because the function prints but does not return
  C: Returns False — print converts the protein to a different format
  D: NameError — protein is not accessible outside the function
answer: B
explanation: |
  The function uses print() instead of return, so it displays the protein
  to the console but returns None by default. Calling .startswith() on None
  raises an AttributeError. Option A would be correct if the function used
  return instead of print. Option C invents a conversion that print does not
  perform. Option D describes a real scoping fact — protein is local — but
  the immediate error is the None return, not a NameError.

---
slug: local-scope-isolation
difficulty: medium
topic: Recognizing that local variables inside a function are invisible to the caller
question: |
  A function computes the GC content of a DNA sequence:

  <python>
  def gc_content(seq):
      gc_count = 0
      for base in seq:
          if base in 'GC':
              gc_count += 1
      fraction = gc_count / len(seq)
      return fraction

  result = gc_content('ATGCGC')
  print(gc_count)
  </python>

  What happens when print(gc_count) executes?
choices:
  A: Prints 4 — gc_count was computed inside the function
  B: Prints 0 — gc_count is reset after the function returns
  C: NameError — gc_count is local to gc_content and does not exist in the caller's scope
  D: Prints 0.667 — gc_count holds the fraction, not the count
answer: C
explanation: |
  Variables created inside a function are local to that function. They exist
  only while the function is executing and are not visible to the caller.
  The function correctly computes gc_count = 4 and returns 0.667, but
  gc_count itself is not accessible outside. Option A assumes local variables
  leak into the caller's namespace. Option B invents a reset mechanism.
  Option D confuses gc_count with fraction. The caller must use the returned
  value (result) to access the computation's output.

---
slug: default-argument-override
difficulty: medium
topic: Using default arguments to provide sensible fallbacks that callers can override
question: |
  A CRISPR guide RNA design function has a default PAM:

  <python>
  def find_targets(seq, pam='NGG'):
      targets = []
      for i in range(len(seq) - len(pam)):
          if matches_pam(seq[i:i+len(pam)], pam):
              targets.append(i)
      return targets

  a = find_targets(genome_seq)
  b = find_targets(genome_seq, 'NGG')
  c = find_targets(genome_seq, 'NNGRRT')
  </python>

  Which calls use the Cas9 PAM <pre>NGG</pre> and which use <pre>NNGRRT</pre>?
choices:
  A: a and b use 'NGG'; c uses 'NNGRRT'
  B: Only b uses 'NGG'; a uses None because no argument was provided
  C: All three use 'NGG' — the default cannot be overridden
  D: a uses 'NGG'; b and c both raise errors for passing a second argument
answer: A
explanation: |
  When no argument is provided for pam, the default 'NGG' is used (call a).
  Passing 'NGG' explicitly (call b) has the same effect. Passing 'NNGRRT'
  (call c) overrides the default with the Cas12a/SaCas9 PAM. Option B
  confuses default values with None — the default is 'NGG', not the absence
  of a value. Option C claims defaults are locked. Option D invents a
  restriction on passing optional arguments. Default arguments let a function
  serve the common case (Cas9) while remaining flexible for alternatives.

---
slug: pure-function-no-side-effects
difficulty: medium
topic: Distinguishing pure functions from those with side effects and understanding why purity aids testing
question: |
  Two implementations of reverse_complement are proposed:

  <python>
  # Version A
  complement_table = {}
  def reverse_complement_a(seq):
      complement_table['last_input'] = seq
      comp = seq.translate(str.maketrans('ATCG', 'TAGC'))
      return comp[::-1]

  # Version B
  def reverse_complement_b(seq):
      comp = seq.translate(str.maketrans('ATCG', 'TAGC'))
      return comp[::-1]
  </python>

  Both return the correct reverse complement. Why is Version B preferred
  for use in a design pipeline?
choices:
  A: Version B is a pure function — same input always gives the same output with no side effects
  B: Version A is slower because dicts are slow
  C: Version B handles lowercase input and Version A does not
  D: Version A returns a different result each time it is called
answer: A
explanation: |
  Version A modifies the global complement_table every time it is called,
  which is a side effect. Pure functions like Version B depend only on their
  inputs and produce only their return value. This makes Version B easier
  to test, compose, and parallelize — you never need to worry about hidden
  state changes. Option B invents a performance issue. Option C is wrong
  because neither handles lowercase. Option D is wrong because A returns the
  same result — it just also mutates external state, which is the problem.

---
slug: function-as-reusable-unit
difficulty: medium
topic: Recognizing when repeated logic should be extracted into a function and how arguments generalize it
question: |
  A student writes code to check for forbidden restriction sites in a
  designed gene:

  <python>
  has_ecori = 'GAATTC' in cds
  has_bsai = 'GGTCTC' in cds
  has_bsai_rc = 'GAGACC' in cds
  has_ecori_rc = 'GAATTC' in cds
  </python>

  They realize the pattern repeats. Which refactoring best captures
  the shared logic?
choices:
  A: |
    <python>
    def has_site(seq, site):
        rc = reverse_complement(site)
        return site in seq or rc in seq
    </python>
  B: |
    <python>
    def has_site(seq):
        return 'GAATTC' in seq or 'GGTCTC' in seq
    </python>
  C: |
    <python>
    def has_site(site):
        return site in cds
    </python>
  D: |
    <python>
    def has_site():
        return True
    </python>
answer: A
explanation: |
  Option A takes both the sequence and the site as parameters, checks the
  site and its reverse complement, and returns a boolean. This works for any
  restriction site on any sequence. Option B hardcodes specific sites, so it
  is not reusable for other enzymes. Option C captures the site parameter
  but relies on cds from the outer scope — it is not self-contained and
  breaks if called from a different context. Option D is trivially useless.
  The key insight is that a well-designed function parameterizes everything
  that varies across use cases.

---
slug: initiate-run-lifecycle
difficulty: medium
topic: Understanding the two-phase pattern of configuring an algorithm object then executing it
question: |
  The RBSChooser class uses an initiate/run pattern:

  <python>
  chooser = RBSChooser()
  chooser.initiate()       # loads proteomics data and builds lookup tables
  result = chooser.run(cds)  # designs an RBS for the given CDS
  </python>

  A student tries to skip initiation:

  <python>
  chooser2 = RBSChooser()
  result2 = chooser2.run(cds)
  </python>

  What is the most likely outcome?
choices:
  A: It works — initiate is optional and run loads data on demand
  B: AttributeError or KeyError — run depends on data structures that initiate creates
  C: result2 is None — run detects missing initialization and returns nothing
  D: TypeError — run cannot be called on an uninitialized object
answer: B
explanation: |
  The initiate method loads proteomics data, parses TSV files, and builds
  internal lookup tables. The run method assumes those data structures exist
  and accesses them directly. Skipping initiate means those attributes were
  never created, so run crashes when it tries to use them. Option A invents
  lazy loading that was not implemented. Option C assumes graceful handling.
  Option D invents a type-level restriction. The two-phase pattern separates
  expensive one-time setup from repeated per-query execution — but the caller
  must honor the contract by calling initiate before run.

---
slug: mutable-default-argument-trap
difficulty: medium
topic: Understanding why mutable default arguments can cause unexpected state sharing between calls
question: |
  A function collects restriction sites to avoid during gene design:

  <python>
  def add_forbidden_site(site, sites=[]):
      sites.append(site)
      return sites

  result1 = add_forbidden_site('GAATTC')
  result2 = add_forbidden_site('GGTCTC')
  </python>

  What is the value of result2?
choices:
  A: "['GGTCTC'] — each call starts with a fresh empty list"
  B: "['GAATTC', 'GGTCTC'] — the default list is shared across calls"
  C: TypeError — you cannot use a list as a default argument
  D: "['GGTCTC', 'GAATTC'] — the list prepends new entries"
answer: B
explanation: |
  In Python, default mutable arguments are created once when the function is
  defined, not on each call. Both calls share the same list object, so the
  first call adds 'GAATTC' and the second adds 'GGTCTC' to that same list.
  This is a well-known Python pitfall. Option A describes the expected
  behavior if a new list were created each call — the fix is to use
  sites=None and create the list inside the function body. Option C is wrong
  because lists are valid defaults (just dangerous). Option D invents
  prepend behavior. In design code, this bug would cause forbidden sites
  from one gene design to silently carry over to the next.

---
slug: class-attribute-vs-instance-attribute
difficulty: medium
topic: Distinguishing data shared across all instances from data specific to one instance
question: |
  A TranscriptDesigner class stores both shared configuration and
  per-design results:

  <python>
  class TranscriptDesigner:
      checkers = [ForbiddenSequenceChecker, HairpinChecker]  # class attribute

      def __init__(self, protein_seq):
          self.protein_seq = protein_seq   # instance attribute
          self.cds = None                  # instance attribute

  designer_a = TranscriptDesigner('MKALIV')
  designer_b = TranscriptDesigner('MPQRST')
  </python>

  Which statement is true?
choices:
  A: designer_a.checkers and designer_b.checkers refer to the same list; their protein_seq values differ
  B: Each designer has its own independent copy of checkers and protein_seq
  C: checkers is only accessible via the class, not through instances
  D: designer_a.protein_seq and designer_b.protein_seq refer to the same string
answer: A
explanation: |
  Class attributes are shared across all instances — designer_a.checkers and
  designer_b.checkers refer to the same list. Instance attributes assigned
  in __init__ are unique to each object, so the protein sequences differ.
  Option B is wrong because class attributes are shared, not copied.
  Option C is wrong because instances can access class attributes through
  normal dot notation. Option D is wrong because protein_seq is assigned
  per-instance. The checkers are shared because all designers use the same
  validation pipeline, while each design works on a different protein.

---
slug: argument-order-and-keyword-args
difficulty: medium
topic: Using positional and keyword arguments to make function calls readable and correct
question: |
  A PCR primer design function has this signature:

  <python>
  def design_primers(template, target_tm=60.0, min_length=18, max_length=30):
      ...
  </python>

  A student calls it four ways:

  <python>
  a = design_primers('ATGCCC...', 55.0, 20, 25)
  b = design_primers('ATGCCC...', max_length=25, target_tm=55.0)
  c = design_primers('ATGCCC...', min_length=20, 55.0)
  d = design_primers(target_tm=55.0, template='ATGCCC...')
  </python>

  Which calls raise a SyntaxError?
choices:
  A: Only c — positional arguments cannot follow keyword arguments
  B: c and d — keyword arguments must match the exact order
  C: Only d — template must be passed positionally
  D: None — Python accepts arguments in any order
answer: A
explanation: |
  Python requires all positional arguments to come before keyword arguments
  in a function call. Call c passes min_length=20 as a keyword then 55.0 as
  a positional argument, which violates this rule and raises SyntaxError.
  Call a uses all positional arguments in order — valid. Call b mixes
  positional and keyword but keywords come last — valid. Call d uses all
  keyword arguments, which can appear in any order — valid. This matters
  in design code where functions have many parameters: keyword arguments
  make calls self-documenting and order-independent.

---
slug: function-composition-pipeline
difficulty: medium
topic: Composing functions so the output of one becomes the input of the next in a processing pipeline
question: |
  A gene design pipeline composes three functions:

  <python>
  def reverse_translate(protein, codon_table):
      '''Returns a DNA CDS encoding the protein.'''
      ...

  def optimize_codons(cds, organism):
      '''Returns an optimized version of the CDS.'''
      ...

  def remove_forbidden_sites(cds, forbidden):
      '''Returns a CDS with forbidden sites silently eliminated.'''
      ...
  </python>

  A student writes:

  <python>
  cds = reverse_translate('MKALI', codon_table)
  optimized = optimize_codons(cds, 'ecoli')
  final = remove_forbidden_sites(optimized, ['GAATTC', 'GGTCTC'])
  </python>

  Another student writes:

  <python>
  final = remove_forbidden_sites(
      optimize_codons(
          reverse_translate('MKALI', codon_table),
          'ecoli'),
      ['GAATTC', 'GGTCTC'])
  </python>

  What is the relationship between these two approaches?
choices:
  A: They produce the same result — the second nests the calls but the execution order and data flow are identical
  B: The nested version runs faster because it avoids creating intermediate variables
  C: The nested version skips the optimize_codons step
  D: They produce different results because nested calls change the evaluation order
answer: A
explanation: |
  Both approaches call the same three functions in the same order with the
  same arguments. The first stores intermediate results in named variables;
  the second nests the calls so each return value is passed directly as an
  argument to the next outer function. Python evaluates innermost calls
  first, so reverse_translate runs first, then optimize_codons, then
  remove_forbidden_sites — the identical pipeline. Option B invents a
  performance difference. Option C misreads the nesting. Option D
  misunderstands evaluation order. Named intermediates are often clearer
  for debugging, but the result is the same.
